digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140360099743376 [label="
 (100, 4)" fillcolor=darkolivegreen1]
	140360118932752 [label=AddmmBackward0]
	140360118930544 -> 140360118932752
	140360318753024 [label="fc.bias
 (4)" fillcolor=lightblue]
	140360318753024 -> 140360118930544
	140360118930544 [label=AccumulateGrad]
	140360118930208 -> 140360118932752
	140360118930208 [label=ReshapeAliasBackward0]
	140360103170048 -> 140360118930208
	140360103170048 [label=MeanBackward1]
	140360103172592 -> 140360103170048
	140360103172592 [label=ReluBackward0]
	140360103170240 -> 140360103172592
	140360103170240 [label=AddBackward0]
	140360103172448 -> 140360103170240
	140360103172448 [label=CudnnBatchNormBackward0]
	140360103171968 -> 140360103172448
	140360103171968 [label=ConvolutionBackward0]
	140360103170720 -> 140360103171968
	140360103170720 [label=ReluBackward0]
	140360103171776 -> 140360103170720
	140360103171776 [label=CudnnBatchNormBackward0]
	140360103172352 -> 140360103171776
	140360103172352 [label=ConvolutionBackward0]
	140360103172880 -> 140360103172352
	140360103172880 [label=ReluBackward0]
	140360103171344 -> 140360103172880
	140360103171344 [label=AddBackward0]
	140360103171728 -> 140360103171344
	140360103171728 [label=CudnnBatchNormBackward0]
	140360103170912 -> 140360103171728
	140360103170912 [label=ConvolutionBackward0]
	140360103170624 -> 140360103170912
	140360103170624 [label=ReluBackward0]
	140360103171488 -> 140360103170624
	140360103171488 [label=CudnnBatchNormBackward0]
	140360103173792 -> 140360103171488
	140360103173792 [label=ConvolutionBackward0]
	140360103172640 -> 140360103173792
	140360103172640 [label=ReluBackward0]
	140360103174272 -> 140360103172640
	140360103174272 [label=AddBackward0]
	140360103173456 -> 140360103174272
	140360103173456 [label=CudnnBatchNormBackward0]
	140360103173600 -> 140360103173456
	140360103173600 [label=ConvolutionBackward0]
	140360103174464 -> 140360103173600
	140360103174464 [label=ReluBackward0]
	140360103173264 -> 140360103174464
	140360103173264 [label=CudnnBatchNormBackward0]
	140360103173696 -> 140360103173264
	140360103173696 [label=ConvolutionBackward0]
	140360103173552 -> 140360103173696
	140360103173552 [label=ReluBackward0]
	140360103175088 -> 140360103173552
	140360103175088 [label=AddBackward0]
	140360103175472 -> 140360103175088
	140360103175472 [label=CudnnBatchNormBackward0]
	140360103176048 -> 140360103175472
	140360103176048 [label=ConvolutionBackward0]
	140360103176096 -> 140360103176048
	140360103176096 [label=ReluBackward0]
	140360103174752 -> 140360103176096
	140360103174752 [label=CudnnBatchNormBackward0]
	140360103176480 -> 140360103174752
	140360103176480 [label=ConvolutionBackward0]
	140360103175808 -> 140360103176480
	140360103175808 [label=ReluBackward0]
	140360103176576 -> 140360103175808
	140360103176576 [label=AddBackward0]
	140360103175568 -> 140360103176576
	140360103175568 [label=CudnnBatchNormBackward0]
	140360103176720 -> 140360103175568
	140360103176720 [label=ConvolutionBackward0]
	140360103177056 -> 140360103176720
	140360103177056 [label=ReluBackward0]
	140360103176768 -> 140360103177056
	140360103176768 [label=CudnnBatchNormBackward0]
	140360103177152 -> 140360103176768
	140360103177152 [label=ConvolutionBackward0]
	140360103175760 -> 140360103177152
	140360103175760 [label=ReluBackward0]
	140360103176240 -> 140360103175760
	140360103176240 [label=AddBackward0]
	140360103177824 -> 140360103176240
	140360103177824 [label=CudnnBatchNormBackward0]
	140360103177488 -> 140360103177824
	140360103177488 [label=ConvolutionBackward0]
	140360103177872 -> 140360103177488
	140360103177872 [label=ReluBackward0]
	140360103177536 -> 140360103177872
	140360103177536 [label=CudnnBatchNormBackward0]
	140360103177968 -> 140360103177536
	140360103177968 [label=ConvolutionBackward0]
	140360103178256 -> 140360103177968
	140360103178256 [label=ReluBackward0]
	140360103178688 -> 140360103178256
	140360103178688 [label=AddBackward0]
	140360103178592 -> 140360103178688
	140360103178592 [label=CudnnBatchNormBackward0]
	140360103178880 -> 140360103178592
	140360103178880 [label=ConvolutionBackward0]
	140360103178736 -> 140360103178880
	140360103178736 [label=ReluBackward0]
	140360103178928 -> 140360103178736
	140360103178928 [label=CudnnBatchNormBackward0]
	140360103179168 -> 140360103178928
	140360103179168 [label=ConvolutionBackward0]
	140360103178496 -> 140360103179168
	140360103178496 [label=ReluBackward0]
	140360103179456 -> 140360103178496
	140360103179456 [label=AddBackward0]
	140360103179552 -> 140360103179456
	140360103179552 [label=CudnnBatchNormBackward0]
	140360103179696 -> 140360103179552
	140360103179696 [label=ConvolutionBackward0]
	140360103179888 -> 140360103179696
	140360103179888 [label=ReluBackward0]
	140360103180032 -> 140360103179888
	140360103180032 [label=CudnnBatchNormBackward0]
	140360103180128 -> 140360103180032
	140360103180128 [label=ConvolutionBackward0]
	140360103179504 -> 140360103180128
	140360103179504 [label=MaxPool2DWithIndicesBackward0]
	140360103180416 -> 140360103179504
	140360103180416 [label=ReluBackward0]
	140360103180512 -> 140360103180416
	140360103180512 [label=CudnnBatchNormBackward0]
	140360103180608 -> 140360103180512
	140360103180608 [label=ConvolutionBackward0]
	140360103180800 -> 140360103180608
	140360319135936 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140360319135936 -> 140360103180800
	140360103180800 [label=AccumulateGrad]
	140360103180560 -> 140360103180512
	140360319129776 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140360319129776 -> 140360103180560
	140360103180560 [label=AccumulateGrad]
	140360103180224 -> 140360103180512
	140360319138416 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140360319138416 -> 140360103180224
	140360103180224 [label=AccumulateGrad]
	140360103180320 -> 140360103180128
	140360319132416 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319132416 -> 140360103180320
	140360103180320 [label=AccumulateGrad]
	140360103180080 -> 140360103180032
	140360319132096 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140360319132096 -> 140360103180080
	140360103180080 [label=AccumulateGrad]
	140360103179936 -> 140360103180032
	140360319131376 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140360319131376 -> 140360103179936
	140360103179936 [label=AccumulateGrad]
	140360103179840 -> 140360103179696
	140360319129936 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319129936 -> 140360103179840
	140360103179840 [label=AccumulateGrad]
	140360103179648 -> 140360103179552
	140360319130656 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140360319130656 -> 140360103179648
	140360103179648 [label=AccumulateGrad]
	140360103179600 -> 140360103179552
	140360319130336 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140360319130336 -> 140360103179600
	140360103179600 [label=AccumulateGrad]
	140360103179504 -> 140360103179456
	140360103179360 -> 140360103179168
	140360319129216 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319129216 -> 140360103179360
	140360103179360 [label=AccumulateGrad]
	140360103179120 -> 140360103178928
	140360319129536 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140360319129536 -> 140360103179120
	140360103179120 [label=AccumulateGrad]
	140360103178976 -> 140360103178928
	140360319129136 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140360319129136 -> 140360103178976
	140360103178976 [label=AccumulateGrad]
	140360103179072 -> 140360103178880
	140360319127616 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319127616 -> 140360103179072
	140360103179072 [label=AccumulateGrad]
	140360103178544 -> 140360103178592
	140360319127936 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140360319127936 -> 140360103178544
	140360103178544 [label=AccumulateGrad]
	140360103178640 -> 140360103178592
	140360319135296 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140360319135296 -> 140360103178640
	140360103178640 [label=AccumulateGrad]
	140360103178496 -> 140360103178688
	140360103178160 -> 140360103177968
	140360319124496 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140360319124496 -> 140360103178160
	140360103178160 [label=AccumulateGrad]
	140360103178112 -> 140360103177536
	140360319124816 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140360319124816 -> 140360103178112
	140360103178112 [label=AccumulateGrad]
	140360103177920 -> 140360103177536
	140360319124416 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140360319124416 -> 140360103177920
	140360103177920 [label=AccumulateGrad]
	140360103177776 -> 140360103177488
	140360319039072 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140360319039072 -> 140360103177776
	140360103177776 [label=AccumulateGrad]
	140360103177728 -> 140360103177824
	140360319125856 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140360319125856 -> 140360103177728
	140360103177728 [label=AccumulateGrad]
	140360103177200 -> 140360103177824
	140360319031072 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140360319031072 -> 140360103177200
	140360103177200 [label=AccumulateGrad]
	140360103177584 -> 140360103176240
	140360103177584 [label=CudnnBatchNormBackward0]
	140360103178448 -> 140360103177584
	140360103178448 [label=ConvolutionBackward0]
	140360103178256 -> 140360103178448
	140360103178304 -> 140360103178448
	140360319126496 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140360319126496 -> 140360103178304
	140360103178304 [label=AccumulateGrad]
	140360103178016 -> 140360103177584
	140360319126416 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140360319126416 -> 140360103178016
	140360103178016 [label=AccumulateGrad]
	140360103177680 -> 140360103177584
	140360319126016 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140360319126016 -> 140360103177680
	140360103177680 [label=AccumulateGrad]
	140360103176912 -> 140360103177152
	140360319027152 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140360319027152 -> 140360103176912
	140360103176912 [label=AccumulateGrad]
	140360103168944 -> 140360103176768
	140360319035952 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140360319035952 -> 140360103168944
	140360103168944 [label=AccumulateGrad]
	140360103176528 -> 140360103176768
	140360319026992 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140360319026992 -> 140360103176528
	140360103176528 [label=AccumulateGrad]
	140360103177008 -> 140360103176720
	140360319025312 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140360319025312 -> 140360103177008
	140360103177008 [label=AccumulateGrad]
	140360103176672 -> 140360103175568
	140360319025392 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140360319025392 -> 140360103176672
	140360103176672 [label=AccumulateGrad]
	140360103175664 -> 140360103175568
	140360319041312 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140360319041312 -> 140360103175664
	140360103175664 [label=AccumulateGrad]
	140360103175760 -> 140360103176576
	140360103175856 -> 140360103176480
	140360319038032 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140360319038032 -> 140360103175856
	140360103175856 [label=AccumulateGrad]
	140360103176144 -> 140360103174752
	140360319038112 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140360319038112 -> 140360103176144
	140360103176144 [label=AccumulateGrad]
	140360103175424 -> 140360103174752
	140360319037712 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140360319037712 -> 140360103175424
	140360103175424 [label=AccumulateGrad]
	140360103174128 -> 140360103176048
	140360319036592 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140360319036592 -> 140360103174128
	140360103174128 [label=AccumulateGrad]
	140360103174944 -> 140360103175472
	140360319036992 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140360319036992 -> 140360103174944
	140360103174944 [label=AccumulateGrad]
	140360103175136 -> 140360103175472
	140360319036512 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140360319036512 -> 140360103175136
	140360103175136 [label=AccumulateGrad]
	140360103174560 -> 140360103175088
	140360103174560 [label=CudnnBatchNormBackward0]
	140360103176864 -> 140360103174560
	140360103176864 [label=ConvolutionBackward0]
	140360103175808 -> 140360103176864
	140360103176816 -> 140360103176864
	140360319039872 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140360319039872 -> 140360103176816
	140360103176816 [label=AccumulateGrad]
	140360103174896 -> 140360103174560
	140360319039552 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140360319039552 -> 140360103174896
	140360103174896 [label=AccumulateGrad]
	140360103172544 -> 140360103174560
	140360319039472 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140360319039472 -> 140360103172544
	140360103172544 [label=AccumulateGrad]
	140360103173216 -> 140360103173696
	140360319035552 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140360319035552 -> 140360103173216
	140360103173216 [label=AccumulateGrad]
	140360103174320 -> 140360103173264
	140360319035712 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140360319035712 -> 140360103174320
	140360103174320 [label=AccumulateGrad]
	140360103174848 -> 140360103173264
	140360319035472 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140360319035472 -> 140360103174848
	140360103174848 [label=AccumulateGrad]
	140360103174512 -> 140360103173600
	140360319033952 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140360319033952 -> 140360103174512
	140360103174512 [label=AccumulateGrad]
	140360103173648 -> 140360103173456
	140360319034272 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140360319034272 -> 140360103173648
	140360103173648 [label=AccumulateGrad]
	140360103173408 -> 140360103173456
	140360319033872 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140360319033872 -> 140360103173408
	140360103173408 [label=AccumulateGrad]
	140360103173552 -> 140360103174272
	140360103173312 -> 140360103173792
	140360319031232 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140360319031232 -> 140360103173312
	140360103173312 [label=AccumulateGrad]
	140360103171200 -> 140360103171488
	140360319031312 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140360319031312 -> 140360103171200
	140360103171200 [label=AccumulateGrad]
	140360103170384 -> 140360103171488
	140360319030832 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140360319030832 -> 140360103170384
	140360103170384 [label=AccumulateGrad]
	140360103170432 -> 140360103170912
	140360319029632 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140360319029632 -> 140360103170432
	140360103170432 [label=AccumulateGrad]
	140360103170000 -> 140360103171728
	140360319029792 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140360319029792 -> 140360103170000
	140360103170000 [label=AccumulateGrad]
	140360103171056 -> 140360103171728
	140360319029472 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140360319029472 -> 140360103171056
	140360103171056 [label=AccumulateGrad]
	140360103171008 -> 140360103171344
	140360103171008 [label=CudnnBatchNormBackward0]
	140360103169664 -> 140360103171008
	140360103169664 [label=ConvolutionBackward0]
	140360103172640 -> 140360103169664
	140360103173120 -> 140360103169664
	140360319033152 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140360319033152 -> 140360103173120
	140360103173120 [label=AccumulateGrad]
	140360103171296 -> 140360103171008
	140360319032752 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140360319032752 -> 140360103171296
	140360103171296 [label=AccumulateGrad]
	140360103170528 -> 140360103171008
	140360319032672 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140360319032672 -> 140360103170528
	140360103170528 [label=AccumulateGrad]
	140360103171920 -> 140360103172352
	140360319028192 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140360319028192 -> 140360103171920
	140360103171920 [label=AccumulateGrad]
	140360103171824 -> 140360103171776
	140360319028352 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140360319028352 -> 140360103171824
	140360103171824 [label=AccumulateGrad]
	140360103170960 -> 140360103171776
	140360319027952 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140360319027952 -> 140360103170960
	140360103170960 [label=AccumulateGrad]
	140360103173744 -> 140360103171968
	140360319029232 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140360319029232 -> 140360103173744
	140360103173744 [label=AccumulateGrad]
	140360103173840 -> 140360103172448
	140360319028592 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140360319028592 -> 140360103173840
	140360103173840 [label=AccumulateGrad]
	140360103171632 -> 140360103172448
	140360319030352 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140360319030352 -> 140360103171632
	140360103171632 [label=AccumulateGrad]
	140360103172880 -> 140360103170240
	140360118930304 -> 140360118932752
	140360118930304 [label=TBackward0]
	140360103172208 -> 140360118930304
	140360362453632 [label="fc.weight
 (4, 512)" fillcolor=lightblue]
	140360362453632 -> 140360103172208
	140360103172208 [label=AccumulateGrad]
	140360118932752 -> 140360099743376
}
