digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140359791138512 [label="
 (100, 4)" fillcolor=darkolivegreen1]
	140360106478320 [label=AddmmBackward0]
	140360106478512 -> 140360106478320
	140360318753024 [label="fc.bias
 (4)" fillcolor=lightblue]
	140360318753024 -> 140360106478512
	140360106478512 [label=AccumulateGrad]
	140360106479472 -> 140360106478320
	140360106479472 [label=ReshapeAliasBackward0]
	140359790560112 -> 140360106479472
	140359790560112 [label=MeanBackward1]
	140359790561792 -> 140359790560112
	140359790561792 [label=ReluBackward0]
	140359790560496 -> 140359790561792
	140359790560496 [label=AddBackward0]
	140359790567648 -> 140359790560496
	140359790567648 [label=CudnnBatchNormBackward0]
	140359790568752 -> 140359790567648
	140359790568752 [label=ConvolutionBackward0]
	140359790563856 -> 140359790568752
	140359790563856 [label=ReluBackward0]
	140359790562128 -> 140359790563856
	140359790562128 [label=CudnnBatchNormBackward0]
	140359790563328 -> 140359790562128
	140359790563328 [label=ConvolutionBackward0]
	140359790569376 -> 140359790563328
	140359790569376 [label=ReluBackward0]
	140359790570864 -> 140359790569376
	140359790570864 [label=AddBackward0]
	140359790572928 -> 140359790570864
	140359790572928 [label=CudnnBatchNormBackward0]
	140359790563280 -> 140359790572928
	140359790563280 [label=ConvolutionBackward0]
	140359790572352 -> 140359790563280
	140359790572352 [label=ReluBackward0]
	140359790560256 -> 140359790572352
	140359790560256 [label=CudnnBatchNormBackward0]
	140359790567456 -> 140359790560256
	140359790567456 [label=ConvolutionBackward0]
	140359790567984 -> 140359790567456
	140359790567984 [label=ReluBackward0]
	140359790572880 -> 140359790567984
	140359790572880 [label=AddBackward0]
	140359790572976 -> 140359790572880
	140359790572976 [label=CudnnBatchNormBackward0]
	140359790573120 -> 140359790572976
	140359790573120 [label=ConvolutionBackward0]
	140359790573504 -> 140359790573120
	140359790573504 [label=ReluBackward0]
	140359790572304 -> 140359790573504
	140359790572304 [label=CudnnBatchNormBackward0]
	140359790565392 -> 140359790572304
	140359790565392 [label=ConvolutionBackward0]
	140359790573024 -> 140359790565392
	140359790573024 [label=ReluBackward0]
	140359790573648 -> 140359790573024
	140359790573648 [label=AddBackward0]
	140359790573456 -> 140359790573648
	140359790573456 [label=CudnnBatchNormBackward0]
	140359790574224 -> 140359790573456
	140359790574224 [label=ConvolutionBackward0]
	140359790574128 -> 140359790574224
	140359790574128 [label=ReluBackward0]
	140359790574800 -> 140359790574128
	140359790574800 [label=CudnnBatchNormBackward0]
	140359790574272 -> 140359790574800
	140359790574272 [label=ConvolutionBackward0]
	140359790574464 -> 140359790574272
	140359790574464 [label=ReluBackward0]
	140359790574416 -> 140359790574464
	140359790574416 [label=AddBackward0]
	140359790575136 -> 140359790574416
	140359790575136 [label=CudnnBatchNormBackward0]
	140359790574848 -> 140359790575136
	140359790574848 [label=ConvolutionBackward0]
	140359790575088 -> 140359790574848
	140359790575088 [label=ReluBackward0]
	140359790575232 -> 140359790575088
	140359790575232 [label=CudnnBatchNormBackward0]
	140359790575520 -> 140359790575232
	140359790575520 [label=ConvolutionBackward0]
	140359790574944 -> 140359790575520
	140359790574944 [label=ReluBackward0]
	140359789199616 -> 140359790574944
	140359789199616 [label=AddBackward0]
	140359789199712 -> 140359789199616
	140359789199712 [label=CudnnBatchNormBackward0]
	140359789199856 -> 140359789199712
	140359789199856 [label=ConvolutionBackward0]
	140359789200048 -> 140359789199856
	140359789200048 [label=ReluBackward0]
	140359789200192 -> 140359789200048
	140359789200192 [label=CudnnBatchNormBackward0]
	140359789200288 -> 140359789200192
	140359789200288 [label=ConvolutionBackward0]
	140359789200480 -> 140359789200288
	140359789200480 [label=ReluBackward0]
	140359789200624 -> 140359789200480
	140359789200624 [label=AddBackward0]
	140359789200720 -> 140359789200624
	140359789200720 [label=CudnnBatchNormBackward0]
	140359789200864 -> 140359789200720
	140359789200864 [label=ConvolutionBackward0]
	140359789201056 -> 140359789200864
	140359789201056 [label=ReluBackward0]
	140359789201200 -> 140359789201056
	140359789201200 [label=CudnnBatchNormBackward0]
	140359789201296 -> 140359789201200
	140359789201296 [label=ConvolutionBackward0]
	140359789200672 -> 140359789201296
	140359789200672 [label=ReluBackward0]
	140359789201584 -> 140359789200672
	140359789201584 [label=AddBackward0]
	140359789201680 -> 140359789201584
	140359789201680 [label=CudnnBatchNormBackward0]
	140359789201824 -> 140359789201680
	140359789201824 [label=ConvolutionBackward0]
	140359789202016 -> 140359789201824
	140359789202016 [label=ReluBackward0]
	140359789202160 -> 140359789202016
	140359789202160 [label=CudnnBatchNormBackward0]
	140359789202256 -> 140359789202160
	140359789202256 [label=ConvolutionBackward0]
	140359789201632 -> 140359789202256
	140359789201632 [label=MaxPool2DWithIndicesBackward0]
	140359789202544 -> 140359789201632
	140359789202544 [label=ReluBackward0]
	140359789202640 -> 140359789202544
	140359789202640 [label=CudnnBatchNormBackward0]
	140359789202736 -> 140359789202640
	140359789202736 [label=ConvolutionBackward0]
	140359789202928 -> 140359789202736
	140360319135936 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140360319135936 -> 140359789202928
	140359789202928 [label=AccumulateGrad]
	140359789202688 -> 140359789202640
	140360319129776 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140360319129776 -> 140359789202688
	140359789202688 [label=AccumulateGrad]
	140359789202352 -> 140359789202640
	140360319138416 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140360319138416 -> 140359789202352
	140359789202352 [label=AccumulateGrad]
	140359789202448 -> 140359789202256
	140360319132416 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319132416 -> 140359789202448
	140359789202448 [label=AccumulateGrad]
	140359789202208 -> 140359789202160
	140360319132096 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140360319132096 -> 140359789202208
	140359789202208 [label=AccumulateGrad]
	140359789202064 -> 140359789202160
	140360319131376 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140360319131376 -> 140359789202064
	140359789202064 [label=AccumulateGrad]
	140359789201968 -> 140359789201824
	140360319129936 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319129936 -> 140359789201968
	140359789201968 [label=AccumulateGrad]
	140359789201776 -> 140359789201680
	140360319130656 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140360319130656 -> 140359789201776
	140359789201776 [label=AccumulateGrad]
	140359789201728 -> 140359789201680
	140360319130336 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140360319130336 -> 140359789201728
	140359789201728 [label=AccumulateGrad]
	140359789201632 -> 140359789201584
	140359789201488 -> 140359789201296
	140360319129216 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319129216 -> 140359789201488
	140359789201488 [label=AccumulateGrad]
	140359789201248 -> 140359789201200
	140360319129536 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140360319129536 -> 140359789201248
	140359789201248 [label=AccumulateGrad]
	140359789201104 -> 140359789201200
	140360319129136 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140360319129136 -> 140359789201104
	140359789201104 [label=AccumulateGrad]
	140359789201008 -> 140359789200864
	140360319127616 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140360319127616 -> 140359789201008
	140359789201008 [label=AccumulateGrad]
	140359789200816 -> 140359789200720
	140360319127936 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140360319127936 -> 140359789200816
	140359789200816 [label=AccumulateGrad]
	140359789200768 -> 140359789200720
	140360319135296 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140360319135296 -> 140359789200768
	140359789200768 [label=AccumulateGrad]
	140359789200672 -> 140359789200624
	140359789200432 -> 140359789200288
	140360319124496 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140360319124496 -> 140359789200432
	140359789200432 [label=AccumulateGrad]
	140359789200240 -> 140359789200192
	140360319124816 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140360319124816 -> 140359789200240
	140359789200240 [label=AccumulateGrad]
	140359789200096 -> 140359789200192
	140360319124416 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140360319124416 -> 140359789200096
	140359789200096 [label=AccumulateGrad]
	140359789200000 -> 140359789199856
	140360319039072 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140360319039072 -> 140359789200000
	140359789200000 [label=AccumulateGrad]
	140359789199808 -> 140359789199712
	140360319125856 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140360319125856 -> 140359789199808
	140359789199808 [label=AccumulateGrad]
	140359789199760 -> 140359789199712
	140360319031072 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140360319031072 -> 140359789199760
	140359789199760 [label=AccumulateGrad]
	140359789199664 -> 140359789199616
	140359789199664 [label=CudnnBatchNormBackward0]
	140359789200384 -> 140359789199664
	140359789200384 [label=ConvolutionBackward0]
	140359789200480 -> 140359789200384
	140359789200528 -> 140359789200384
	140360319126496 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140360319126496 -> 140359789200528
	140359789200528 [label=AccumulateGrad]
	140359789199952 -> 140359789199664
	140360319126416 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140360319126416 -> 140359789199952
	140359789199952 [label=AccumulateGrad]
	140359789199904 -> 140359789199664
	140360319126016 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140360319126016 -> 140359789199904
	140359789199904 [label=AccumulateGrad]
	140359789199520 -> 140359790575520
	140360319027152 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140360319027152 -> 140359789199520
	140359789199520 [label=AccumulateGrad]
	140359790575424 -> 140359790575232
	140360319035952 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140360319035952 -> 140359790575424
	140359790575424 [label=AccumulateGrad]
	140359790575184 -> 140359790575232
	140360319026992 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140360319026992 -> 140359790575184
	140359790575184 [label=AccumulateGrad]
	140359790575568 -> 140359790574848
	140360319025312 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140360319025312 -> 140359790575568
	140359790575568 [label=AccumulateGrad]
	140359790574560 -> 140359790575136
	140360319025392 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140360319025392 -> 140359790574560
	140359790574560 [label=AccumulateGrad]
	140359790575328 -> 140359790575136
	140360319041312 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140360319041312 -> 140359790575328
	140359790575328 [label=AccumulateGrad]
	140359790574944 -> 140359790574416
	140359790574992 -> 140359790574272
	140360319038032 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140360319038032 -> 140359790574992
	140359790574992 [label=AccumulateGrad]
	140359790573360 -> 140359790574800
	140360319038112 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140360319038112 -> 140359790573360
	140359790573360 [label=AccumulateGrad]
	140359790574368 -> 140359790574800
	140360319037712 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140360319037712 -> 140359790574368
	140359790574368 [label=AccumulateGrad]
	140359790574704 -> 140359790574224
	140360319036592 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140360319036592 -> 140359790574704
	140359790574704 [label=AccumulateGrad]
	140359790573600 -> 140359790573456
	140360319036992 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140360319036992 -> 140359790573600
	140359790573600 [label=AccumulateGrad]
	140359790568176 -> 140359790573456
	140360319036512 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140360319036512 -> 140359790568176
	140359790568176 [label=AccumulateGrad]
	140359790573696 -> 140359790573648
	140359790573696 [label=CudnnBatchNormBackward0]
	140359790574608 -> 140359790573696
	140359790574608 [label=ConvolutionBackward0]
	140359790574464 -> 140359790574608
	140359790574896 -> 140359790574608
	140360319039872 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140360319039872 -> 140359790574896
	140359790574896 [label=AccumulateGrad]
	140359790574176 -> 140359790573696
	140360319039552 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140360319039552 -> 140359790574176
	140359790574176 [label=AccumulateGrad]
	140359790574512 -> 140359790573696
	140360319039472 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140360319039472 -> 140359790574512
	140359790574512 [label=AccumulateGrad]
	140359790574032 -> 140359790565392
	140360319035552 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140360319035552 -> 140359790574032
	140359790574032 [label=AccumulateGrad]
	140359790573936 -> 140359790572304
	140360319035712 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140360319035712 -> 140359790573936
	140359790573936 [label=AccumulateGrad]
	140359790573792 -> 140359790572304
	140360319035472 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140360319035472 -> 140359790573792
	140359790573792 [label=AccumulateGrad]
	140359790573552 -> 140359790573120
	140360319033952 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140360319033952 -> 140359790573552
	140359790573552 [label=AccumulateGrad]
	140359790567552 -> 140359790572976
	140360319034272 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140360319034272 -> 140359790567552
	140359790567552 [label=AccumulateGrad]
	140359790573984 -> 140359790572976
	140360319033872 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140360319033872 -> 140359790573984
	140359790573984 [label=AccumulateGrad]
	140359790573024 -> 140359790572880
	140359790569472 -> 140359790567456
	140360319031232 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140360319031232 -> 140359790569472
	140359790569472 [label=AccumulateGrad]
	140359790572400 -> 140359790560256
	140360319031312 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140360319031312 -> 140359790572400
	140359790572400 [label=AccumulateGrad]
	140359790569088 -> 140359790560256
	140360319030832 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140360319030832 -> 140359790569088
	140359790569088 [label=AccumulateGrad]
	140359790562464 -> 140359790563280
	140360319029632 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140360319029632 -> 140359790562464
	140359790562464 [label=AccumulateGrad]
	140359790565008 -> 140359790572928
	140360319029792 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140360319029792 -> 140359790565008
	140359790565008 [label=AccumulateGrad]
	140359790567504 -> 140359790572928
	140360319029472 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140360319029472 -> 140359790567504
	140359790567504 [label=AccumulateGrad]
	140359790573840 -> 140359790570864
	140359790573840 [label=CudnnBatchNormBackward0]
	140359790568272 -> 140359790573840
	140359790568272 [label=ConvolutionBackward0]
	140359790567984 -> 140359790568272
	140359790569664 -> 140359790568272
	140360319033152 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140360319033152 -> 140359790569664
	140359790569664 [label=AccumulateGrad]
	140359790563808 -> 140359790573840
	140360319032752 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140360319032752 -> 140359790563808
	140359790563808 [label=AccumulateGrad]
	140359790560064 -> 140359790573840
	140360319032672 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140360319032672 -> 140359790560064
	140359790560064 [label=AccumulateGrad]
	140359790570960 -> 140359790563328
	140360319028192 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140360319028192 -> 140359790570960
	140359790570960 [label=AccumulateGrad]
	140359790561744 -> 140359790562128
	140360319028352 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140360319028352 -> 140359790561744
	140359790561744 [label=AccumulateGrad]
	140359790564576 -> 140359790562128
	140360319027952 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140360319027952 -> 140359790564576
	140359790564576 [label=AccumulateGrad]
	140359790571392 -> 140359790568752
	140360319029232 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140360319029232 -> 140359790571392
	140359790571392 [label=AccumulateGrad]
	140359790572544 -> 140359790567648
	140360319028592 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140360319028592 -> 140359790572544
	140359790572544 [label=AccumulateGrad]
	140359790572064 -> 140359790567648
	140360319030352 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140360319030352 -> 140359790572064
	140359790572064 [label=AccumulateGrad]
	140359790569376 -> 140359790560496
	140359790567120 -> 140360106478320
	140359790567120 [label=TBackward0]
	140359790568944 -> 140359790567120
	140360362453632 [label="fc.weight
 (4, 512)" fillcolor=lightblue]
	140360362453632 -> 140359790568944
	140359790568944 [label=AccumulateGrad]
	140360106478320 -> 140359791138512
}
